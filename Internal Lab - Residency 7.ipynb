{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyfMmMnPJjvn"
   },
   "source": [
    "## Train a simple convnet on the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjcGOJhcJjvp"
   },
   "source": [
    "In this, we will see how to deal with image data and train a convnet for image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rames\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.layers import Reshape, Dense, Flatten, Dropout,MaxPooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jR0Pl2XjJjvq"
   },
   "source": [
    "### Load the  `fashion_mnist`  dataset\n",
    "\n",
    "** Use keras.datasets to load the dataset **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr75v_UYJjvs"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTI42-0qJjvw"
   },
   "source": [
    "### Find no.of samples are there in training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2sf67VoJjvx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28) \n",
      "y_train shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape:\", x_train.shape, \"\\ny_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zewyDcBlJjv1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (10000, 28, 28) \n",
      "y_train shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape:\", x_test.shape, \"\\ny_train shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WytT2eRnJjv4"
   },
   "source": [
    "### Find dimensions of an image in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XycQGBSGJjv5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of train images (28, 28)\n",
      "Dimension of test images (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of train images\",x_train.shape[1:3])\n",
    "print(\"Dimension of test images\",x_test.shape[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jtdZ7RqJjv8"
   },
   "source": [
    "### Convert train and test labels to one hot vectors\n",
    "\n",
    "** check `keras.utils.to_categorical()` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAD3q5I6Jjv9"
   },
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgHSCXy3JjwA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "First 5 examples now are:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print('First 5 examples now are: ', y_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xO5BRBzBJjwD"
   },
   "source": [
    "### Normalize both the train and test image data from 0-255 to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fUQpMHxJjwE"
   },
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Okwo_SB5JjwI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "da5-DwgrJjwM"
   },
   "source": [
    "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPGVQ-JJJjwN"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Reshape((784,),input_shape=(28,28,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFRRTJq8JjwQ"
   },
   "source": [
    "### Import the necessary layers from keras to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWTZYnKSJjwR"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=100,activation=\"relu\"))\n",
    "model.add(Dense(units=10,activation=\"softmax\"))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 1.0085 - acc: 0.6933 - val_loss: 0.7196 - val_acc: 0.7579\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.6394 - acc: 0.7902 - val_loss: 0.6112 - val_acc: 0.7928\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.5657 - acc: 0.8116 - val_loss: 0.5653 - val_acc: 0.8073\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.5269 - acc: 0.8226 - val_loss: 0.5365 - val_acc: 0.8170\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.5023 - acc: 0.8288 - val_loss: 0.5210 - val_acc: 0.8201\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(x_train, y_train, \n",
    "          validation_data=(x_test, y_test), \n",
    "          epochs=5,\n",
    "          batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 75us/step\n",
      "Accuracy:  0.8200999965965747\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(x=x_test,y=y_test,batch_size=10)\n",
    "print(\"Accuracy: \", accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try using BatchNormaalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(200, activation='sigmoid', name='Layer_1'))\n",
    "\n",
    "model.add(Dense(100, activation='sigmoid', name='Layer_2'))\n",
    "\n",
    "model.add(Dense(100, activation='relu', name='Layer_3'))\n",
    "\n",
    "#Output layer\n",
    "model.add(keras.layers.Dense(10, activation='softmax', name='Output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create optimizer with non-default learning rate\n",
    "sgd_optimizer = optimizers.SGD(lr=0.03)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 1.1073 - acc: 0.6460 - val_loss: 0.6364 - val_acc: 0.7651\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.5803 - acc: 0.7878 - val_loss: 0.5335 - val_acc: 0.8067\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.4988 - acc: 0.8192 - val_loss: 0.4800 - val_acc: 0.8274\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.4586 - acc: 0.8348 - val_loss: 0.4565 - val_acc: 0.8339\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.4326 - acc: 0.8434 - val_loss: 0.4294 - val_acc: 0.8428\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.4116 - acc: 0.8530 - val_loss: 0.4227 - val_acc: 0.8462\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.3970 - acc: 0.8575 - val_loss: 0.4125 - val_acc: 0.8481\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 106us/step - loss: 0.3864 - acc: 0.8613 - val_loss: 0.3956 - val_acc: 0.8576\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.3738 - acc: 0.8643 - val_loss: 0.3925 - val_acc: 0.8579\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.3603 - acc: 0.8704 - val_loss: 0.3869 - val_acc: 0.8590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b58322908>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,          \n",
    "          validation_data=(x_test,y_test),\n",
    "          epochs=10,\n",
    "          batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_2 (Reshape)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "Layer_1 (Dense)              (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "Layer_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "Layer_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 191,346\n",
      "Trainable params: 189,778\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 104us/step\n",
      "Accuracy:  0.8589999940991402\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(x=x_test,y=y_test,batch_size=10)\n",
    "print(\"Accuracy: \", accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try with Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "#Normalize the data\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "#Hidden layers\n",
    "model.add(Dense(200, activation='relu', name='Layer_1'))\n",
    "model.add(Dense(100, activation='relu', name='Layer_2'))\n",
    "\n",
    "#Dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Hidden layers\n",
    "model.add(Dense(60, activation='relu', name='Layer_3'))\n",
    "model.add(Dense(30, activation='relu', name='Layer_4'))\n",
    "\n",
    "#Dropout layer\n",
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer\n",
    "model.add(keras.layers.Dense(10, activation='softmax', name='Output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 33s 547us/step - loss: 0.8050 - acc: 0.7275 - val_loss: 0.4868 - val_acc: 0.8291\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 28s 461us/step - loss: 0.5878 - acc: 0.8044 - val_loss: 0.4472 - val_acc: 0.8450\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 26s 436us/step - loss: 0.5258 - acc: 0.8269 - val_loss: 0.4523 - val_acc: 0.8409\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 26s 434us/step - loss: 0.4918 - acc: 0.8382 - val_loss: 0.4188 - val_acc: 0.8494\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 26s 430us/step - loss: 0.4725 - acc: 0.8433 - val_loss: 0.4027 - val_acc: 0.8573\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 26s 436us/step - loss: 0.4523 - acc: 0.8491 - val_loss: 0.4014 - val_acc: 0.8601\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 26s 437us/step - loss: 0.4377 - acc: 0.8531 - val_loss: 0.3842 - val_acc: 0.8634\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 26s 434us/step - loss: 0.4304 - acc: 0.8552 - val_loss: 0.3991 - val_acc: 0.8596\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 26s 442us/step - loss: 0.4197 - acc: 0.8576 - val_loss: 0.3662 - val_acc: 0.8708\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 26s 439us/step - loss: 0.4126 - acc: 0.8596 - val_loss: 0.3793 - val_acc: 0.8684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b5ba3d3c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(x_train,y_train,          \n",
    "          validation_data=(x_test,y_test),\n",
    "          epochs=10,\n",
    "          batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_3 (Reshape)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "Layer_1 (Dense)              (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "Layer_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "Layer_3 (Dense)              (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "Layer_4 (Dense)              (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                310       \n",
      "=================================================================\n",
      "Total params: 188,436\n",
      "Trainable params: 186,868\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 109us/step\n",
      "Accuracy:  0.8683999938964844\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(x=x_test,y=y_test,batch_size=10)\n",
    "print(\"Accuracy: \", accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "model.add(Dense(200, activation='sigmoid', name='Layer_1'))\n",
    "\n",
    "model.add(Dense(100, activation='sigmoid', name='Layer_2'))\n",
    "\n",
    "model.add(Dense(100, activation='relu', name='Layer_3'))\n",
    "\n",
    "#Output layer\n",
    "model.add(keras.layers.Dense(10, activation='softmax', name='Output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = SGD(lr=0.001,momentum=.8)\n",
    "model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 1.9828 - acc: 0.3978 - val_loss: 1.4800 - val_acc: 0.5679\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 1.1472 - acc: 0.6392 - val_loss: 0.9359 - val_acc: 0.6870\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.8222 - acc: 0.7103 - val_loss: 0.7575 - val_acc: 0.7157\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.6997 - acc: 0.7395 - val_loss: 0.6847 - val_acc: 0.7451\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.6445 - acc: 0.7564 - val_loss: 0.6412 - val_acc: 0.7604\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 8s 142us/step - loss: 0.6110 - acc: 0.7702 - val_loss: 0.6217 - val_acc: 0.7695\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.5873 - acc: 0.7826 - val_loss: 0.6051 - val_acc: 0.7700\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.5680 - acc: 0.7925 - val_loss: 0.5881 - val_acc: 0.7882\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.5513 - acc: 0.8009 - val_loss: 0.5709 - val_acc: 0.7899\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.5364 - acc: 0.8081 - val_loss: 0.5531 - val_acc: 0.7981\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.5232 - acc: 0.8143 - val_loss: 0.5456 - val_acc: 0.8036\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.5101 - acc: 0.8178 - val_loss: 0.5364 - val_acc: 0.8063\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.4989 - acc: 0.8230 - val_loss: 0.5196 - val_acc: 0.8139\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.4879 - acc: 0.8274 - val_loss: 0.5160 - val_acc: 0.8160\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.4789 - acc: 0.8307 - val_loss: 0.5116 - val_acc: 0.8148\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.4704 - acc: 0.8330 - val_loss: 0.4961 - val_acc: 0.8212\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.4623 - acc: 0.8360 - val_loss: 0.4928 - val_acc: 0.8232\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.4550 - acc: 0.8381 - val_loss: 0.4828 - val_acc: 0.8267\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.4488 - acc: 0.8413 - val_loss: 0.4803 - val_acc: 0.8274\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.4417 - acc: 0.8448 - val_loss: 0.4715 - val_acc: 0.8306\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.4368 - acc: 0.8451 - val_loss: 0.4700 - val_acc: 0.8324\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.4309 - acc: 0.8472 - val_loss: 0.4633 - val_acc: 0.8345\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.4258 - acc: 0.8494 - val_loss: 0.4601 - val_acc: 0.8323\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.4217 - acc: 0.8507 - val_loss: 0.4589 - val_acc: 0.8339\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.4172 - acc: 0.8514 - val_loss: 0.4521 - val_acc: 0.8395\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.4131 - acc: 0.8530 - val_loss: 0.4515 - val_acc: 0.8368\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.4095 - acc: 0.8555 - val_loss: 0.4506 - val_acc: 0.8411\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.4062 - acc: 0.8568 - val_loss: 0.4419 - val_acc: 0.8419\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.4019 - acc: 0.8573 - val_loss: 0.4401 - val_acc: 0.8430\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 0.3987 - acc: 0.8588 - val_loss: 0.4406 - val_acc: 0.8423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b5c390dd8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(x_train,y_train,          \n",
    "          validation_data=(x_test,y_test),\n",
    "          epochs=30,\n",
    "          batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_4 (Reshape)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "Layer_1 (Dense)              (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "Layer_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "Layer_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 188,210\n",
      "Trainable params: 188,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 87us/step\n",
      "Accuracy:  0.8422999946773052\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(x=x_test,y=y_test,batch_size=10)\n",
    "print(\"Accuracy: \", accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "Lambda = 1e3\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Reshape((784,),input_shape=(28,28,)))\n",
    "\n",
    "model.add(Dense(200, activation='sigmoid', name='Layer_1'))\n",
    "\n",
    "model.add(Dense(100, activation='relu', name='Layer_2'))\n",
    "\n",
    "model.add(Dense(100, activation='relu', name='Layer_3'))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(10, activation='softmax', kernel_regularizer=regularizers.l2(Lambda)))\n",
    "\n",
    "sgd_optimizer = SGD(lr=0.001,momentum=.8)\n",
    "model.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 31.9983 - acc: 0.0983 - val_loss: 2.3049 - val_acc: 0.1000\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 2.3045 - acc: 0.0987 - val_loss: 2.3049 - val_acc: 0.1000\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 2.3044 - acc: 0.0994 - val_loss: 2.3072 - val_acc: 0.1000\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 2.3044 - acc: 0.0963 - val_loss: 2.3048 - val_acc: 0.1000\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 2.3045 - acc: 0.0985 - val_loss: 2.3048 - val_acc: 0.1000\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 2.3045 - acc: 0.0980 - val_loss: 2.3037 - val_acc: 0.1000\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 2.3044 - acc: 0.0983 - val_loss: 2.3040 - val_acc: 0.1000\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 2.3044 - acc: 0.0977 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 2.3044 - acc: 0.0995 - val_loss: 2.3044 - val_acc: 0.1000\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 2.3044 - acc: 0.0982 - val_loss: 2.3067 - val_acc: 0.1000\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 2.3044 - acc: 0.0987 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 2.3045 - acc: 0.0982 - val_loss: 2.3043 - val_acc: 0.1000\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 2.3044 - acc: 0.0977 - val_loss: 2.3054 - val_acc: 0.1000\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 2.3045 - acc: 0.0974 - val_loss: 2.3044 - val_acc: 0.1000\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 2.3044 - acc: 0.0978 - val_loss: 2.3053 - val_acc: 0.1000\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 2.3044 - acc: 0.0974 - val_loss: 2.3045 - val_acc: 0.1000\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 2.3044 - acc: 0.0994 - val_loss: 2.3039 - val_acc: 0.1125\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 2.3045 - acc: 0.0977 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 2.3045 - acc: 0.0992 - val_loss: 2.3038 - val_acc: 0.1000\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 2.3045 - acc: 0.0985 - val_loss: 2.3052 - val_acc: 0.1000\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 2.3045 - acc: 0.0978 - val_loss: 2.3031 - val_acc: 0.1000\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 2.3044 - acc: 0.0984 - val_loss: 2.3038 - val_acc: 0.1000\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 2.3044 - acc: 0.0973 - val_loss: 2.3046 - val_acc: 0.1000\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 2.3045 - acc: 0.0973 - val_loss: 2.3044 - val_acc: 0.1000\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 2.3044 - acc: 0.0976 - val_loss: 2.3061 - val_acc: 0.1728\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 2.3045 - acc: 0.0974 - val_loss: 2.3054 - val_acc: 0.1000\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 2.3045 - acc: 0.0980 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 2.3045 - acc: 0.0975 - val_loss: 2.3045 - val_acc: 0.1000\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 2.3045 - acc: 0.0992 - val_loss: 2.3044 - val_acc: 0.1000\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 2.3044 - acc: 0.0976 - val_loss: 2.3033 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b5bb3de80>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "model.fit(x_train,y_train,          \n",
    "          validation_data=(x_test,y_test),\n",
    "          epochs=30,\n",
    "          batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_5 (Reshape)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "Layer_1 (Dense)              (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "Layer_2 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "Layer_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 188,210\n",
      "Trainable params: 188,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 107us/step\n",
      "Accuracy:  0.10000000193715096\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(x=x_test,y=y_test,batch_size=10)\n",
    "print(\"Accuracy: \", accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### with adam optimizer it is giving best accurancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C18AoS7eJjwU"
   },
   "source": [
    "### Build a model \n",
    "\n",
    "** with 2 Conv layers having `32 3*3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DORCLgSwJjwV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ju69vKdIJjwX"
   },
   "source": [
    "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2hAP94vJjwY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lGTA3bfEJjwa"
   },
   "source": [
    "### Now, to the above model, lets add Data Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6gX8n5SJjwb"
   },
   "source": [
    "### Import the ImageDataGenrator from keras and fit the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cbz4uHBuJjwc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl-8dOo7Jjwf"
   },
   "source": [
    "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpI1_McYJjwg",
    "outputId": "6722631e-c925-448c-c780-93a3100249bc",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datagen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-ad5fc72e3025>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"off\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datagen' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "gen = datagen.flow(x_train[0:1], batch_size=1)\n",
    "for i in range(1, 6):\n",
    "    plt.subplot(1,5,i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
    "    plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmPl5yE8Jjwm"
   },
   "source": [
    "### Run the above model using fit_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44ZnDdJYJjwn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwQQW5iOJjwq"
   },
   "source": [
    "###  Report the final train and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1SrtBEPJjwq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBwVWNQC2qZD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KXqmUDW2rM1"
   },
   "source": [
    "## **DATA AUGMENTATION ON CIFAR10 DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8mja6OgQ3L18"
   },
   "source": [
    "One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set. Aside from gathering more instances from the wild that are representative of the distinction task, we want to develop a set of methods that enhance the data we already have. There are many ways to augment existing datasets and produce more robust models. In the image domain, these are done to utilize the full power of the convolutional neural network, which is able to capture translational invariance. This translational invariance is what makes image recognition such a difficult task in the first place. You want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions that are of interest to the vision task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HzVTPUM3WZJ"
   },
   "source": [
    "### **Import neessary libraries for data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPM558TX4KMb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6hicLwP4SqY"
   },
   "source": [
    "### **Load CIFAR10 dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQ1WzrXd4WNk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9Pht1ggHuiT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3n28ccU6Hp6s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JN3vYYhK4W0u"
   },
   "source": [
    "### **Create a data_gen funtion to genererator with image rotation,shifting image horizontally and vertically with random flip horizontally.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJbekTKi4cmM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e-SLtUhC4dK2"
   },
   "source": [
    "### **Prepare/fit the generator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSw8Bv2_4hb0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gYyF-P8O4jQ8"
   },
   "source": [
    "### **Generate 5 images for 1 of the image of CIFAR10 train dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXug4z234mwQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "R7_InternalLab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
